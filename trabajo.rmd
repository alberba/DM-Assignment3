---
title: "Assignment 3"
author: "Santiago Rattenbach, Àngel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(e1071)
    library(class)
    library(gmodels)
    library(tree)
    library(FSelector)
    library(partykit)
    library(party)
    library(RWeka)
    library(caret)
    library(C50)
    library(rpart.plot)
    library(MLmetrics)
  })
})
```

## The Data

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

### Independent Variables

As we can see, the dataset contains 45,000 observations and 14 variables. The variables are as follows:

1. **person age:** Age of the person (numeric)
2. **person gender:** Gender of the person (categorical: female, male)
3. **person education:** Highest education level (categorical: Associate, Bachelor, Doctorate, High School, 
Master)
4. **person income:** Annual income (numeric)
5. **person emp exp:** Years of employment experience (integer)
6. **person home ownership:** Home ownership status (categorical: MORTGAGE, OTHER, OWN, RENT)
7. **loan amnt:** Loan amount requested (numeric)
8. **loan intent:** Purpose of the loan (categorical: DEBTCONSOLIDATION, EDUCATION, HOME-IMPROVEMENT, 
MEDICAL, PERSONAL, VENTURE)
9. **loan int rate:** Loan interest rate (numeric)
10. **loan percent income:** Loan amount as a percentage of annual income (numeric)
11. **cb person cred hist length:** Length of credit history in years (numeric)
12. **credit score:** Credit score of the person (integer)
13. **previous loan defaults on file:** Indicator of previous loan defaults (categorical: No, Yes)

### Target Variable

- **loan_status**: The status of the loan (integer 1 = approved; 0 = rejected)

```{r}
# Create the bar plot
ggplot(data, aes(x = factor(loan_status, labels = c("Rejected", "Approved")))) +
  geom_bar(fill = "skyblue") +
  labs(title = "Distribution of loan approval status",
       x = "Loan status",
       y = "Quantity")
```

Most loans end up getting denied, we aim to understand why that is the case and build a model that predicts 
the outcome as much as possible.

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```

#### Numerical Variables

- **person_age:** This dataset includes adults, so the minimum age is 20 years. The mean is 27.76 years, 
meaning most of the samples in the dataset are from young people. On the other hand, we can see that 
there are very few samples for people over 50 years old. Therefore, we will remove observations with 
ages above 50 as we consider them outliers.

```{r}
data <- subset(data, person_age <= 50)
```

- **person_income:** This dataset ranges from annual values of $8,000 to $7,200,766. We will assume 
that samples with annual incomes above $250,000 would not face any difficulty obtaining a loan within 
the scope of this dataset.

```{r}
data <- subset(data, person_income < 250000)
```

- **person_emp_exp:** This variable ranges from 0 to 123 years. The latter is quite an outlier, especially 
considering that the maximum accepted age is 50 years. Similarly, by removing observations with ages above 
50 years, we also eliminate observations with more than 50 years of work experience.

- **loan_amnt**: The loan amounts range from $500 to $35,000, with no apparent outliers at first glance.

- **loan_int_rate**: The loan interest rate ranges from 5.42% to 20%. It is likely that as the interest rate
 increases, the probability of the loan being approved also increases, since the bank would be taking on
 greater risk but could achieve higher profit.
 
- **loan_percent_income**: As we can see, there are samples where the loan percentage over annual income is 
0%, which is impossible. So, we will check how many samples meet this condition.


```{r}
aux <- subset(data, loan_percent_income == 0)
head(aux)
```

We see that there are 3 samples with a loan percentage over annual income of 0%. This is impossible because 
if the percentage is 0%, the loan amount would be $0. Therefore, we will proceed to remove them.

```{r}
data <- subset(data, loan_percent_income != 0)
```

- **cb_person_cred_hist_length**: The length of credit history ranges from 2 to 30 years. There don't seem 
to be any outliers.

- **credit_score**: The credit score ranges from 390 to 850. There don't seem to be any outliers.

Let's see how the dataset looks after removing the outliers:

```{r}
summary(data)
```
```{r}
# List of numerical variables:
numeric <- c('person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 
                    'loan_int_rate', 'loan_percent_income', 
                    'cb_person_cred_hist_length', 'credit_score')


# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

As we can see, except for the credit_score, most of the numerical variables do not follow a normal 
distribution and are right-skewed. Later, we'll modify the values to make them follow a normal distribution.

#### Categorical

```{r}
# List of categorical variables:
categories <- c('person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **gender:** As we can see, the majority of the applicants are male, however, the difference is not very
significant so there isn't much information to be gained from these values.

- **education:** The number of applicants with a master's degree is approximately half the number of people 
in any category, except for those with a doctorate, who are significantly fewer than that. This variable may 
be useful in predicting the loan status.
It can be observed that people who have a higher level of education are more.prone to apply for loans than
those with a lower level of education..

- **home ownership:** The majority of applicants are renters, followed by mortgage holders. There are few 
applicants who own their homes and even fewer who own other types of homes. This variable may also be useful 
in predicting the loan status.

- **loan intent:** The most common loan intents are both education and medical bills, followed by debt
consolidation, venture and personal loans. Fewer people take out loans for home improvement.
This variable may also be useful in predicting the loan status.

### Data Correlations

#### Numerical Variables

```{r}
numeric_Corr <- data[, c('person_age', 'person_income', 'person_emp_exp',
                        'loan_amnt', 'loan_int_rate', 'loan_percent_income',
                        'cb_person_cred_hist_length', 'credit_score', 'loan_status')]

ggcorr(numeric_Corr, label = TRUE)
```
As we can see, 'credit_score' has little influence, as its correlations with the other variables are 
virtually null and it has no correlation with the target variable. Therefore, we will proceed to remove it. 
On the other hand, 'person_age', 'person_emp_exp', and 'cb_person_cred_hist_length' have a very high 
correlation with each other, so we will remove 'person_emp_exp' and 'cb_person_cred_hist_length' to avoid 
multicollinearity.

```{r}
# Supression of the variables
data_Mod <- data[, -c(5, 11, 12)]

# Visualize the changes
names(data_Mod)
```

```{r}
# Compare the correlation matrix of the new dataset:
ggcorr(data_Mod, label = TRUE, label_round = 2)
```

Now we can see that the variable 'person_age' has little to no correlation with other variables or with the
target variable. Therefore, it will not contribute to the model, so we will proceed to remove it.

```{r}
data_Mod <- data_Mod[, -c(1)]
```
 
### Bivariate Analysis

#### Numeric Variables

Comparing the loan status with the all numerical variables, we can see if there is any relationship between them.

```{r}
p1 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = person_income), fill = "#FFB6C1", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Person Income", x = "Loan Status", y = "Person Income") +
  theme_minimal()

print(p1)
```

Loan approval is inversely proportional to the applicant's income, higher incomes tend to have their loans 
rejected. This information doesn't seem to make much sense to us, as we would think that loans taken by
people with higher income should be both safer and more profitable, however that is not the case here.


```{r}
p2 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_amnt), fill = "violet", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Amount", x = "Loan Status", y = "Loan Amount") +
  theme_minimal()

print(p2)
```

As we can see, those who have their loans approved tend to request higher loan amounts, that is, the higher 
the loan amount, the more likely it is to be approved. This makes sense as the bank would make more profit.

```{r}
p3 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_percent_income), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Percent Income", x = "Loan Status", y = "Loan Percent Income") +
  theme_minimal()

print(p3)
```

In this case, those who have a higher loan percentage over annual income are more likely to have their
loans approved. This is probably because the bank makes more profit from these loans with more interest rate, 
even though they are riskier than those with a lower percentage.

```{r}
p4 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_int_rate), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Int Rate", x = "Loan Status", y = "Loan Int Rate") +
  theme_minimal()

print(p4)
```
While the median interest rate for approved loans is higher than for rejected loans, approved loans have a
wider range of interest rates, which means that the outcome of the request is not solely based on the
interest rate.

#### Categorical Variables

Comparing the loan status with every categorical variable, we can see if there is any relationship between them.

```{r}
p1 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_gender, y = loan_status), fill = "lightblue") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Gender", x = "Person Gender", y = "Loan Status")

print(p1)
```

```{r}
p2 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_education, y = loan_status), fill = "lightpink") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Education", x = "Person Education", y = "Loan Status")

print(p2)
```

```{r}
p3 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_home_ownership, y = loan_status), fill = "lightgreen") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Home Ownership", x = "Person Home Ownership", y = "Loan Status")

print(p3)
```

```{r}
p4 <- ggplot(data_Mod) +
  geom_violin(aes(x = loan_intent, y = loan_status), fill = "lavender") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Loan Intent", x = "Loan Intent", y = "Loan Status")

print(p4)
```

```{r}
p5 <- ggplot(data_Mod) +
  geom_violin(aes(x = previous_loan_defaults_on_file, y = loan_status), fill = "peachpuff") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Previous Loan Defaults", x = "Previous Loan Defaults", y = "Loan Status")

print(p5)
```

It can be seen that the variable 'previous_loan_defaults_on_file' is key to predicting the loan status, as
applicants who have had previous loan defaults are always rejected. On the other hand, the rest of the
attributes do not seem to have a significant impact on the loan status (at least not easily seen via this
plots) regardless of their value, as all of their profiles are accepted around 25% of the times.

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data_Mod))
```

As we can see, there are no missing values in the dataset, so we can proceed to the next stage.

### Key Features

As we have already seen with the violin plots, the 'previous_loan_defaults_on_file' variable is key to
predicting the loan status, as applicants who have had previous loan defaults are always rejected.

Analyzing the correlation between the variables, it is observed that person_income, loan_int_rate,
loan_percent_income and, to a lesser extent, loan_amnt have a high correlation with the loan_status variable,
therefore they are also important variables to consider.

### PreProcessing Data

#### Checking Normality (Skewness)

Now we will check the normality of the numerical variables to see if they follow a normal distribution.
In case they do not, we will apply transformations to make them follow a normal distribution.

- **person_income**:
  
```{r}
skewness(data_Mod$person_income)
```

We get a skewness of 1.31, so we need to apply another transformation, to improve the distribution.

```{r}
skewness(log(data_Mod$person_income))
```

We see that by applying this logarithmic transformation, we improve the distribution of the 'person_income'
variable, going from 1.31 to -0.18, which is a significant improvement.

- **loan_amnt**:

```{r}
skewness(data_Mod$loan_amnt)
```

We get a skewness of 1.17, so we need to apply a transformation to improve it.

```{r}
skewness(sqrt(data_Mod$loan_amnt))
skewness(log(data_Mod$loan_amnt))
```

As we can see, both solve the skewness problem, but the square root transformation is more effective.

- **loan_int_rate**:

```{r}
skewness(data_Mod$loan_int_rate)
```

We get a skewness of 0.21, so we do not need to apply any transformation.

- **loan_percent_income**:

```{r}
skewness(data_Mod$loan_percent_income)
```

We get a skewness of 1.03, so we need to apply another transformation.

```{r}
skewness(sqrt(data_Mod$loan_percent_income))
skewness(log(data_Mod$loan_percent_income))
```

As we can see, both help improve the skewness problem, but the square root transformation is more effective
and falls within the acceptable range.

#### Creation of a better skewed dataset

With all the transformations made, we proceed to create a new dataset with the transformed variables,
as their distribution follows the normal distribution more closely:

```{r}
data_Transformed <- data_Mod

data_Transformed$person_income <- log(data_Mod$person_income)
data_Transformed$loan_amnt <- sqrt(data_Mod$loan_amnt)
data_Transformed$loan_percent_income <- sqrt(data_Mod$loan_percent_income)
```

## Model Building

### Sorting Variables

To better handle the data, we find it convenient to reorder the columns, placing the numerical columns 
first, followed by the categorical ones, and finally the target variable.

```{r}
# Identify numeric and factor columns
numeric_columns <- sapply(data_Transformed, is.numeric)
factor_columns <- sapply(data_Transformed, is.factor)

# Exclude the target variable from the numeric and factor columns
numeric_columns <- numeric_columns & names(data_Transformed) != "loan_status"

# Order the columns: first the numeric ones, then the factor ones, and finally the target variable
ordered_columns <- c(names(data_Transformed)[numeric_columns], 
                     names(data_Transformed)[factor_columns], 
                     "loan_status")

# Reorder DataFrame
data_Transformed <- data_Transformed[, ordered_columns]
```

### Data Normalization

We ensure that all the numerical variables are normalized to the same scale, so that there isn't one with a
greater influence than the others.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Identify the numerical columns
numerical_columns <- sapply(data_Transformed, is.numeric)

# Apply a transformation (e.g., normalization) only to the numerical columns
data_Normalized <- data_Transformed
# Normalize the numerical data
data_Normalized[, numerical_columns] <- as.data.frame(lapply(data_Transformed[, numerical_columns], normalize))

# Confirm that normalization worked
summary(data_Normalized)
```

### Train-Test Split

Before splitting into train and test sets, it is important to know that, for k-Nearest Neighbors and Naive 
Bayes, all variables need to be numeric. For categorical variables, we use one-hot encoding to convert them 
into numeric variables, where each category becomes a new binary column.

```{r}
# Convert categorical variables into numerical ones with one-hot encoding
data_Normalized <- model.matrix(~ . - 1, data = data_Normalized)

# Convert the result to a data frame
data_Normalized <- as.data.frame(data_Normalized)
```

```{r}
## To reproduce the calculations
seeds <- c(1357, 2468, 3579)
set.seed(seeds[1])

## Create an index to partition the data set
ind <- sample(2, nrow(data_Normalized), replace=TRUE, prob=c(0.80, 0.20))

data_Normalized.train <- data_Normalized[ind==1,]
data_Normalized.test <- data_Normalized[ind==2,]
```

We have split the data into a training set and a test set, with 80% of the data in the training set and 20%
in the test set. Let's check that the partitions have been done correctly.

```{r}
nrow(data_Normalized)
nrow(data_Normalized.train)
nrow(data_Normalized.test)
```

## Classification using Nearest Neighbors

### Predictions

We will start building the KNN model, where the independent variables range from position 1 to 19, and 
loan_status is in position 20. We have chosen k=3 as the default value.

```{r}
## Build the classifier
data_Normalized.knn <- knn(data_Normalized.train[, 1:19], data_Normalized.test[, 1:19], cl = data_Normalized.train[, 20], k = 3)

confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn)
F1_Score(data_Normalized.knn, data_Normalized.test$loan_status, positive = "0")
```

From the table, we can see that the model is generally accurate, with an error margin of 0.108 (89.2% 
accuracy). On the other hand, if we focus on the 'accepted' column, the accuracy is 0.724, which is quite 
low. This means that the model may be more prone to approving loans for cases where it should reject them. 
This could have a **negative impact on the bank**, as it would have to take on more risks.

To understand why it is important to convert categorical variables into numerical ones instead of ignoring 
them, we will create a model using only the numerical variables. These range from the first to the
fifth column.

```{r}
## Build the classifier
data_Normalized.knn2 <- knn(data_Normalized.train[, 1:4], data_Normalized.test[, 1:4], cl = data_Normalized.train[, 20], k = 3)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn2)
```

As we can see, the model is less accurate, which makes sense since the categorical variables also influence 
the decision of whether a loan is approved or not.

Now, we will try with two different k values: 5 and 2.

```{r}
## Build the classifier
data_Normalized.knn3 <- knn(data_Normalized.train[, 1:19], data_Normalized.test[, 1:19], cl = data_Normalized.train[, 20], k = 2)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn3)
```

```{r}
data_Normalized.knn4 <- knn(data_Normalized.train[, 1:19], data_Normalized.test[, 1:19], cl = data_Normalized.train[, 20], k = 5)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn4)
```

In both new models, we don't see any substantial improvements that would justify changing the original model. 
Additionally, the original model is the one that takes the least time to run.

## Classification using Naive Bayes

```{r}
classifier.NB <- naiveBayes(x = data_Normalized.train[, 1:19], y = data_Normalized.train[, 20])
data_Normalized.NB <- predict(classifier.NB, data_Normalized.test[, 1:19])

confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.NB)
F1_Score(data_Normalized.NB, data_Normalized.test$loan_status, positive = "0")
```

In the case of the Naive Bayes model, we can see that the accuracy is around 10% lower than the KNN model, 
which is not a great result. This may be due to the fact that in the Naive Bayes model, it is assumed that
the variables are conditionally independent, when in reality they are not.

It should also be noted that, as we can see in the table, the model takes a more conservative approach when 
it comes to evaluating loan profiles, therefore it is less likely to approve the loan request. Because of 
this, this model may be preferred to k-NN in this case, as the bank may prefer taking a safer approach by 
rejecting more requests even though they should be valid rather than accepting more that shouldn't be valid.

## Classification using Decision Trees

In order to use decision trees, we need to convert the numerical variables into categorical ones.

### Conversion of Numerical Variables 

```{r}
# Conversion of numerical variables into categorical ones, divided into intervals:
data_Mod$person_income <- cut(data$person_income, breaks = c(0, 50000, 100000, 150000, 200000, 250000), labels = c("0-50k", "50k-100k", "100k-150k", "150k-200k", "200k-250k"))
data_Mod$loan_amnt <- cut(data$loan_amnt, breaks = c(0, 5000, 10000, 20000, 35000), labels = c("0-5k", "5k-10k", "10k-20k", "20k-35k"))
data_Mod$loan_int_rate <- cut(data$loan_int_rate, breaks = c(5, 8.5, 11, 13, 20), labels = c("5-8.5", "8.5-11", "11-13", "13-20"))
data_Mod$loan_percent_income <- cut(data$loan_percent_income, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.67), labels = c("0-0.1", "0.1-0.2", "0.2-0.3", "0.3-0.4", "0.4-0.67"))

# The same is done with the dependent variable
data_Mod$loan_status <- as.factor(data_Mod$loan_status)
```

### ID3 Algorithm

```{r}
# Create an index to partition the data set
ind1 <- sample(2, nrow(data_Mod), replace = TRUE, prob = c(0.80, 0.20))
data_Mod.train1 <- data_Mod[ind1 == 1, ]
data_Mod.test1 <- data_Mod[ind1 == 2, ]

# Model creation
ID3_1 <- J48(loan_status ~ person_income + loan_amnt + loan_int_rate + loan_percent_income, data = data_Mod.train1)
```

Once we have built the tree, let's see how it performs with the test set.

```{r}
data_Mod.ID31 <- predict(ID3_1, data_Mod.test1)
confusionMatrix(data_Mod.ID31, data_Mod.test1$loan_status)
```

As we can see, we have an accuracy of 81.5%, which is not bad, but there still may be room for improvement.

Now, we will add the rest of the categorical variables, based on the second model, with more optimal 
intervals:


```{r}
# Model creation
ID3_2 <- J48(loan_status ~ ., data = data_Mod.train1)

data_Mod.ID32 <- predict(ID3_2, data_Mod.test1)
confusionMatrix(data_Mod.ID32, data_Mod.test1$loan_status)
F1_Score(data_Mod.ID32, data_Mod.test1$loan_status, positive = "0")
```

As we have seen, the model has improved significantly. On the other hand, we believe the tree may be 
overloaded with nodes, so let's see which variables are the most important in order to optimize the model 
without compromising accuracy.

```{r}
information.gain(loan_status ~ ., data = data_Mod)
```

We generated a new tree using only the variables that are truly important. In this case, we have removed the
3 variables with the least importance (person_gender, person_education, and loan_amnt):

```{r}
# Create model with selected variables
ID2_Sel <- J48(loan_status ~ previous_loan_defaults_on_file + loan_percent_income + person_home_ownership + person_income + loan_intent, 
               data = data_Mod.train1)

data_Mod.ID2Sel <- predict(ID2_Sel, data_Mod.test1)
confusionMatrix(data_Mod.ID2Sel, data_Mod.test1$loan_status)
F1_Score(data_Mod.ID2Sel, data_Mod.test1$loan_status, positive = "0")
```

The accuracy has dropped to 87.3%, a decrease of 3%. Nevertheless, we believe the model has been 
significantly optimized as, by removing 3 variables, it produces far fewer nodes. Depending on the purpose,
a 3% drop may not be worth it. Not only that, but the amount of incorrect approvals has also decreased,
which is beneficial for the bank.


### C5.0 Algorithm

```{r}
C50 <- C5.0(loan_status ~ ., data = data_Mod.train1)
data_Mod.C50 <- predict(C50, data_Mod.test1)
confusionMatrix(data_Mod.C50, data_Mod.test1$loan_status)
F1_Score(data_Mod.C50, data_Mod.test1$loan_status, positive = "0")
```

We observe a slight improvement compared to the ID3 model, but it is not significant. This may be due to the
fact that not all partitions are distributed in the same way, so the C5.0 model may benefit in this case. 
However, the improvement is almost negligible.


### CART Algorithm

```{r}
classifier.CART <- ctree(loan_status ~ ., data = data_Mod.train1)
data_Mod.CART <- predict(classifier.CART, data_Mod.test1)
confusionMatrix(data_Mod.CART, data_Mod.test1$loan_status)
F1_Score(data_Mod.CART, data_Mod.test1$loan_status, positive = "0")
```

The results are similar to those obtained with C5.0; they are slightly worse, but with a very small margin.

### Decision Tree using rpart

```{r}
classifier.rpart <- rpart(loan_status ~ ., data = data_Mod.train1, method = "class")
data_Mod.rpart <- predict(classifier.rpart, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.rpart, data_Mod.test1$loan_status)
F1_Score(data_Mod.rpart, data_Mod.test1$loan_status, positive = "0")
```

Yet again, the results given by the decision tree are similar to the ones given by previous tree models, so
there isn't much to say about it.

### Decision Tree using Tree

```{r}
classifier.tree <- tree(loan_status ~ ., data = data_Mod.train1)
data_Mod.tree <- predict(classifier.tree, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.tree, data_Mod.test1$loan_status)
F1_Score(data_Mod.tree, data_Mod.test1$loan_status, positive = "0")
```

The results are still quite similar, although this model performs slightly worse than the previous ones.

## Classification without omitting variables

To conduct more tests, let's check if what we did earlier by removing variables that had a high correlation
with other independent variables was correct. For that, we will compare one of the previous models, but 
without omitting any variables.

### Sorting variables

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)

# Identify the numeric and factor columns
numerical_columns <- sapply(data, is.numeric)
factor_columns <- sapply(data, is.factor)

# Exclude the target variable
numerical_columns <- numerical_columns & names(data) != "loan_status"

# Sort the columns: first the numeric ones, then the factor ones, and finally the target variable
ordered_columns <- c(names(data)[numerical_columns], 
                     names(data)[factor_columns], 
                     "loan_status")

# Sort the DataFrame
data <- data[, ordered_columns]
```

### Data Normalization

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

numerical_columns <- sapply(data, is.numeric)

data_NoModified <- data
# Normalize the numerical data
data_NoModified[, numerical_columns] <- as.data.frame(lapply(data[, numerical_columns], normalize))
# Confirm that normalization worked
summary(data_NoModified)
```

### Train-Test Split

```{r}
# Convert categorical variables into dummy variables
data_NoModified <- model.matrix(~ . - 1, data = data_NoModified)

# Convert the result to a DataFrame
data_NoModified <- as.data.frame(data_NoModified)
```

```{r}
# To reproduce the calculations
seeds <- c(1357, 2468, 3579)
set.seed(seeds[1])

# Create an index to partition the data set
ind <- sample(2, nrow(data_NoModified), replace=TRUE, prob=c(0.80, 0.20))

data_NoModified.train <- data_NoModified[ind==1,]
data_NoModified.test <- data_NoModified[ind==2,]
```

```{r}
nrow(data_NoModified)
nrow(data_NoModified.train)
nrow(data_NoModified.test)
```

```{r}
# Build the classifier
data_NoModified.knn <- knn(data_NoModified.train[, 1:23], data_NoModified.test[, 1:23], cl = data_NoModified.train[, 24], k = 3)

confusionMatrix(as.factor(data_NoModified.test$loan_status), data_NoModified.knn)
F1_Score(data_NoModified.knn, data_NoModified.test$loan_status, positive = "0")
```

As we can see, both the F1 score and accuracy decrease, and the model itself is less efficient. By including 
variables with high correlation, unnecessary data is added, which slows down the model.

## Conclusion

Our main objective was to understand what factors influence loan approvals and to create a model that 
could accurately predict whether a loan should be approved or not.

As we explored the data, we discovered some oddities, like applicants with unrealistically high ages and 
incomes, or loan percentages that didn't make sense. To ensure our models were working with reliable 
information, we cleaned up these anomalies so the data reflected realistic scenarios.

Using different plots and transformations we examined how each variable behaved and how it related to the 
loan approval status. This exploratory analysis revealed some key insights:

- **Influential Factors:** Variables like previous_loan_defaults_on_file were strong indicators of whether a 
loan would be approved.

- **Data Skewness:** Many numerical variables were skewed to the right, which meant we needed to adjust them 
to better fit the models we planned to use. 

As we said earlier, we applied transformations like logarithms and square roots to improve the skewness, 
because those models work better when data is normally distributed.

We also looked at how the variables were correlated with each other. We found that some variables, such as 
person_age, person_emp_exp and cb_person_cred_hist_length, were highly correlated. To avoid issues like 
multicollinearity, which can negatively impact model performance, we decided to remove these variables.
Then, we chose to trim the persone_age variable (and its correlated variables), as it didn't seem to have a
significant impact on the loan status.

With our data cleaned up and normalized, we were ready to build and evaluate several classification models.

**1. k-Nearest Neighbors (KNN):**

We normalized the data and converted categorical variables into numerical format using one-hot encoding. 
We tried different values of k and found that k=3 gave us the best results.

**2. Naive Bayes:**

We didn't have to tweak any parameters as the Naive Bayes algorithm is very straightforward.

**3. Decision Trees:**

We explored various algorithms, including ID3, C5.0 and CART. We used packages like rpart and tree.

The ID3 algorithm, when using all variables, reached about 90.3% accuracy.
Simplifying the model by focusing on the most important features slightly reduced accuracy to 87.3%, but made 
the model more efficient and easier to interpret.

The C5.0, CART and the other algorithms offered slight improvements over ID3, but the gains were minimal.

To ensure our decision to exclude certain variables, we ran additional tests without omitting any features. 
The results confirmed that including highly correlated variables didn't improve model performance and 
sometimes even reduced accuracy due to added complexity, so we were right to exclude them.

### Testing different seeds of train/test

We decided to test our models using different random seeds and varying the train-test split ratios.
This approach helped us understand how sensitive our models were to changes in the data partitioning.

#### Seed 1

#### 0.8 Train - 0.2 Test

- **KNN-3:** 89.1% accuracy. F1-Score: 0.93 
- **KNN-2:** 88.2% accuracy.
- **KNN-5:** 89.5% accuracy. 
  
- **Naive Bayes:** 74.7% accuracy. F1-Score: 0.806
  
- **ID3:** 87.2% accuracy. F1-Score: 0.921
- **C5.0:** 90.2% accuracy. F1-Score: 0.937
- **CART:** 90.2% accuracy. F1-Score: 0.938
- **rpart:** 89.4% accuracy. F1-Score: 0.933
- **tree:** 88.2% accuracy. F1-Score: 0.924

- **KNN-No_Modified:** 88.5% accuracy. F1-Score: 0.927

#### 0.7 Train - 0.3 Test

- **KNN-3:** 89.2% accuracy. F1-Score: 0.931 
- **KNN-2:** 87.9% accuracy.
- **KNN-5:** 89.6% accuracy. 
  
- **Naive Bayes:** 74.65% accuracy. F1-Score: 0.806
  
- **ID3:** 87.4% accuracy. F1-Score: 0.922
- **C5.0:** 90.7% accuracy. F1-Score: 0.941
- **CART:** 90.5% accuracy. F1-Score: 0.940
- **rpart:** 90.2% accuracy. F1-Score: 0.938
- **tree:** 89.1% accuracy. F1-Score: 0.931

- **KNN-No_Modified:** 88.4% accuracy. F1-Score: 0.927

#### 0.9 Train - 0.1 Test

- **KNN-3:** 88.8% accuracy. F1-Score: 0.929 
- **KNN-2:** 87.5% accuracy.
- **KNN-5:** 89.8% accuracy. 
  
- **Naive Bayes:** 75.7% accuracy. F1-Score: 0.815
  
- **ID3:** 87.4% accuracy. F1-Score: 0.923
- **C5.0:** 90.3% accuracy. F1-Score: 0.938
- **CART:** 90.4% accuracy. F1-Score: 0.938
- **rpart:** 89.7% accuracy. F1-Score: 0.934
- **tree:** 88.5% accuracy. F1-Score: 0.926

- **KNN-No_Modified:** 88.7% accuracy. F1-Score: 0.929

#### Seed 2

- **KNN-3:** 89.1% accuracy. F1-Score: 0.93
- **KNN-2:** 87.6% accuracy.
- **KNN-5:** 89.5% accuracy. 
  
- **Naive Bayes:** 74.7% accuracy. F1-Score: 0.807
  
- **ID3:** 87.4% accuracy. F1-Score: 0.922
- **C5.0:** 90.3% accuracy. F1-Score: 0.938
- **CART:** 90.3% accuracy. F1-Score: 0.938
- **rpart:** 89.7% accuracy. F1-Score: 0.935
- **tree:** 88.6% accuracy. F1-Score: 0.927

- **KNN-No_Modified:** 88.4% accuracy. F1-Score: 0.926

#### Seed 3

- **KNN-3:** 89.7% accuracy. F1-Score: 0.935
- **KNN-2:** 87.8% accuracy.
- **KNN-5:** 90% accuracy. 
  
- **Naive Bayes:** 74% accuracy. F1-Score: 0.801
  
- **ID3:** 87.4% accuracy. F1-Score: 0.923
- **C5.0:** 90.2% accuracy. F1-Score: 0.938
- **CART:** 90% accuracy. F1-Score: 0.936
- **rpart:** 89.4% accuracy. F1-Score: 0.933
- **tree:** 87.9% accuracy. F1-Score: 0.923

- **KNN-No_Modified:** 89.1% accuracy. F1-Score: 0.93

We can observe that our models are quite stable, as the results are very similar across different seeds 
and train-test splits. 

### Choosing the best model

After extensive testing with different seeds and train-test splits, it became evident that the Decision Tree
using the C5.0 algorithm consistently outperformed the other models. 

Across various seeds and data splits, the C5.0 model maintained an accuracy of around 90.2% to 90.7% and an
F1-Score within the range of 0.938 to 0.941.

With that being said, the entity employing the model may choose to prioritize safety over profit, so they
would be more interested in using the ID3 model, which is a bit more conservative in its loan approval
decisions. This can also be said about the Naive Bayes model, which is significantly less accurate than
other models (up to around 10%), but is also considerably more conservative, minimizing wrong approvals.

### Our Learnings

We realized how crucial it is to start with clean, accurate data. Handling outlier samples and missing values
can dramatically affect model performance. The same can be said about our efforts in normalizing
distributions and removing redundant variables.

By experimenting with different algorithms, we saw firsthand how models like Naive Bayes, KNN, and decision
trees behave under various conditions. 

### References

- [R Documentation] (https://www.rdocumentation.org/): Used to understand some of the functions and packages
  used in our analysis.
- [ChatGPT] (chatgpt.com): Used for help in making sense of the data, translation of some text originally
  written in Spanish and removal of certain warnings generated by library functions.
- [RMD files from class]: Used as a reference for the structure of the document and as examples of
  model implementation.
