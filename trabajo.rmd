---
title: "Assignment 3"
author: "Santiago Rattenbach, Angel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(e1071)
    library(class)
    library(gmodels)
    library(tree)
    library(FSelector)
    library(partykit)
    library(party)
    library(RWeka)
    library(caret)
    library(C50)
    library(rpart.plot)
    library(MLmetrics)
  })
})
```

## The Data

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

### Independent Variables

As we can see, the dataset contains 45,000 observations and 14 variables. The variables are as follows:

1. **person age:** Age of the person (numeric)
2. **person gender:** Gender of the person (categorical: female, male)
3. **person education:** Highest education level (categorical: Associate, Bachelor, Doctorate, High School, 
Master)
4. **person income:** Annual income (numeric)
5. **person emp exp:** Years of employment experience (integer)
6. **person home ownership:** Home ownership status (categorical: MORTGAGE, OTHER, OWN, RENT)
7. **loan amnt:** Loan amount requested (numeric)
8. **loan intent:** Purpose of the loan (categorical: DEBTCONSOLIDATION, EDUCATION, HOME-IMPROVEMENT, 
MEDICAL, PERSONAL, VENTURE)
9. **loan int rate:** Loan interest rate (numeric)
10. **loan percent income:** Loan amount as a percentage of annual income (numeric)
11. **cb person cred hist length:** Length of credit history in years (numeric)
12. **credit score:** Credit score of the person (integer)
13. **previous loan defaults on file:** Indicator of previous loan defaults (categorical: No, Yes)

### Target Variable

- **loan_status**: The status of the loan (integer 1 = approved; 0 = rejected)

```{r}
# Crear el gráfico de barras
ggplot(data, aes(x = factor(loan_status, labels = c("Rechazado", "Aprobado")))) +
  geom_bar(fill = "skyblue") +
  labs(title = "Distribution of loan approval status",
       x = "Loan status",
       y = "Quantity")
```

Most loans end up getting denied, we aim to understand why that is the case and build a model that predicts 
the outcome as much as possible.

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```

#### Numerical Variables

- **person_age:** This dataset includes adults, so the minimum age is 20 years. The mean is 27.76 years, 
meaning most of the samples in the dataset are from young people. On the other hand, we can see that 
there are very few samples for people over 50 years old. Therefore, we will remove observations with 
ages above 50 as we consider them outliers.

```{r}
data <- subset(data, person_age <= 50)
```

- **person_income:** This dataset ranges from annual values of $8,000 to $7,200,766. We will assume 
that samples with annual incomes above $250,000 would not face any difficulty obtaining a loan within 
the scope of this dataset.

```{r}
data <- subset(data, person_income < 250000)
```

- **person_emp_exp:** This variable ranges from 0 to 123 years. The latter is quite an outlier, especially 
considering that the maximum accepted age is 50 years. Similarly, by removing observations with ages above 
50 years, we also eliminate observations with more than 50 years of work experience.

- **loan_amnt**: The loan amounts range from $500 to $35,000, with no apparent outliers at first glance.

- **loan_int_rate**: The loan interest rate ranges from 5.42% to 20%. It is likely that as the interest rate
 increases, the probability of the loan being approved also increases, since the bank would be taking on
 greater risk but could achieve higher profit.
 
- **loan_percent_income**: As we can see, there are samples where the loan percentage over annual income is 
0%, which is impossible. So, we will check how many samples meet this condition.


```{r}
aux <- subset(data, loan_percent_income == 0)
head(aux)
```

We see that there are 3 samples with a loan percentage over annual income of 0%. This is impossible because 
if the percentage is 0%, the loan amount would be $0. Therefore, we will proceed to remove them.

```{r}
data <- subset(data, loan_percent_income != 0)
```

- **cb_person_cred_hist_length**: The length of credit history ranges from 2 to 30 years. There don't seem 
to be any outliers.

- **credit_score**: The credit score ranges from 390 to 850. There don't seem to be any outliers.

Let's see how the dataset looks after removing the outliers:

```{r}
summary(data)
```
```{r}
# List of numerical variables:
numeric <- c('person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 
                    'loan_int_rate', 'loan_percent_income', 
                    'cb_person_cred_hist_length', 'credit_score')


# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

As we can see, except for the credit_score, most of the numerical variables do not follow a normal 
distribution and are right-skewed. Later, we'll modify the values to make them follow a normal distribution.

#### Categorical

```{r}
# List of categorical variables:
categories <- c('person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **gender:** As we can see, the majority of the applicants are male, however, the difference is not very
significant so there isn't much information to be gained from these values.

- **education:** The number of applicants with a master's degree is approximately half the number of people 
in any category, except for those with a doctorate, who are significantly fewer than that. This variable may 
be useful in predicting the loan status.
It can be observed that people who have a higher level of education are more.prone to apply for loans than
those with a lower level of education..

- **home ownership:** The majority of applicants are renters, followed by mortgage holders. There are few 
applicants who own their homes and even fewer who own other types of homes. This variable may also be useful 
in predicting the loan status.

- **loan intent:** The most common loan intents are both education and medical bills, followed by debt
consolidation, venture and personal loans. Fewer people take out loans for home improvement.
This variable may also be useful in predicting the loan status.

### Data Correlations

#### Numerical Variables

```{r}
numeric_Corr <- data[, c('person_age', 'person_income', 'person_emp_exp',
                        'loan_amnt', 'loan_int_rate', 'loan_percent_income',
                        'cb_person_cred_hist_length', 'credit_score', 'loan_status')]

ggcorr(numeric_Corr, label = TRUE)
```
As we can see, 'credit_score' has little influence, as its correlations with the other variables are 
virtually null, and it has no correlation with the target variable. Therefore, we will proceed to remove it. 
On the other hand, 'person_age', 'person_emp_exp', and 'cb_person_cred_hist_length' have a very high 
correlation with each other, so we will remove 'person_emp_exp' and 'cb_person_cred_hist_length' to avoid 
multicollinearity.

```{r}
# Supression of the variables
data_Mod <- data[, -c(5, 11, 12)]

# Verify the changes
names(data_Mod)
```

```{r}
# Compare the correlation matrix of the new dataset:
ggcorr(data_Mod, label = TRUE, label_round = 2)
```

Now we can see that all the correlations are appropriate, so we can proceed to the next stage.
 
### Bivariate Analysis

#### Numeric Variables

Comparing the loan status with the all numerical variables to see if there is any relationship between them.

```{r} 
p1 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = person_age), fill = "violet", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Person Age", x = "Loan Status", y = "Person Age") +
  theme_minimal()

print(p1)
```

As we can saw earlier and we can confirm with this graph, the age of the person does not seem to have a 
significant impact on the loan status, as the median age is very similar for both approved and rejected loans.

```{r}
p2 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = person_income), fill = "#FFB6C1", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Person Income", x = "Loan Status", y = "Person Income") +
  theme_minimal()

print(p2)
```

Loan approval is inversly proportional to the applicant's income, higher incomes tend to have their loans 
rejected. This information doesn't seem to make much sense to us, as you would think that loans taken by
people with higher income should be both safer and more profitable, however that is not the case here.


```{r}
p3 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_amnt), fill = "violet", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Amount", x = "Loan Status", y = "Loan Amount") +
  theme_minimal()

print(p3)
```

As we can see, those who have their loans approved tend to request higher loan amounts, that is, the higher 
the loan amount, the more likely it is to be approved. This makes sense as the bank would make more profit.

```{r}
p4 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_percent_income), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Percent Income", x = "Loan Status", y = "Loan Percent Income") +
  theme_minimal()

print(p4)
```

In this case, those who have a higher loan percentage over annual income are more likely to have their
loans approved. This is probably because the bank makes more profit from these loans with more interest rate, 
even though they are riskier than those with a lower percentage.

```{r}
p5 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_int_rate), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Int Rate", x = "Loan Status", y = "Loan Int Rate") +
  theme_minimal()

print(p5)
```


#### Categorical Variables

Comparing the loan status with the all categorical variables to see if there is any relationship between them.

```{r}
p1 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_gender, y = loan_status), fill = "lightblue") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Gender", x = "Person Gender", y = "Loan Status")

print(p1)
```

```{r}
p2 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_education, y = loan_status), fill = "lightpink") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Education", x = "Person Education", y = "Loan Status")

print(p2)
```

```{r}
p3 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_home_ownership, y = loan_status), fill = "lightgreen") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Home Ownership", x = "Person Home Ownership", y = "Loan Status")

print(p3)
```

```{r}
p4 <- ggplot(data_Mod) +
  geom_violin(aes(x = loan_intent, y = loan_status), fill = "lavender") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Loan Intent", x = "Loan Intent", y = "Loan Status")

print(p4)
```

```{r}
p5 <- ggplot(data_Mod) +
  geom_violin(aes(x = previous_loan_defaults_on_file, y = loan_status), fill = "peachpuff") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Previous Loan Defaults", x = "Previous Loan Defaults", y = "Loan Status")

print(p5)
```

It can be seen that the variable 'previous_loan_defaults_on_file' is key to predicting the loan status, as
applicants who have had previous loan defaults are always rejected. On the other hand, the rest of the
attributes do not seem to have a significant impact on the loan status (at least not easily seen via this
plots) regardless of their value, as all of their profiles are accepted around 25% of the times.

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data_Mod))
```

As we can see, there are no missing values in the dataset, so we can proceed to the next stage.

### Key Features

As we have already seen with the vioin plots, the 'previous_loan_defaults_on_file' variable is key to
predicting the loan status, as applicants who have had previous loan defaults are always rejected.

Analyzing the correlation between the variables, it is observed that both person_income and loan_amnt,
loan_int_rate, and loan_percent_income have a high correlation, therefore they are also important
variables to consider.

### PreProcessing Data

#### Checking Normality (Skewness)

Now we will check the normality of the numerical variables to see if they follow a normal distribution.
In case they do not, we will apply transformations to make them follow a normal distribution.

- person_age:
  
```{r}
skewness(data_Mod$person_age)
```

We get a skewness of 1.37, so we need to apply a transformation to improve the distribution.

```{r}
skewness(log(data_Mod$person_age - 19))
```

We see that by applying this logarithmic transformation, we improve the distribution of the 'person_age'
variable, since the minimum age of the people is 20 years, so we subtract 19 to avoid the logarithm of 0.

- person_income:
  
```{r}
skewness(data_Mod$person_income)
```

We get a skewness of 1.31, so we need to apply another transformation, to improve the distribution.

```{r}
skewness(log(data_Mod$person_income))
```

We see that by applying this logarithmic transformation, we improve the distribution of the 'person_income'
variable, going from 1.31 to -0.18, which is a significant improvement.

- loan_amnt:

```{r}
skewness(data_Mod$loan_amnt)
```

We get a skewness of 1.17, so we need to apply a transformation to improve it.

```{r}
skewness(sqrt(data_Mod$loan_amnt))
skewness(log(data_Mod$loan_amnt))
```

As we can see, both solve the asymmetry problem, but the square root transformation is more effective.

- loan_int_rate:

```{r}
skewness(data_Mod$loan_int_rate)
```

We get a skewness of 0.21, so we do not need to apply any transformation.

- loan_percent_income:

```{r}
skewness(data_Mod$loan_percent_income)
```

We get a skewness of 1.03, so we need to apply another transformation.

```{r}
skewness(sqrt(data_Mod$loan_percent_income))
skewness(log(data_Mod$loan_percent_income))
```

As we can see, both solve the skewness problem, but the square root transformation is more effective.

#### Creation of a better skewed dataset

With all the transformations made, we proceed to create a new dataset with the transformed variables,
as they better follow the normal distribution:

```{r}
data_Transformed <- data_Mod

data_Transformed$person_age <- log(data_Mod$person_age - 19)
data_Transformed$person_income <- log(data_Mod$person_income)
data_Transformed$loan_amnt <- sqrt(data_Mod$loan_amnt)
data_Transformed$loan_percent_income <- sqrt(data_Mod$loan_percent_income)
```

## Model Building

### Sorting Variables

```{r}
# Identify numeric and factor columns
numeric_columns <- sapply(data_Transformed, is.numeric)
factor_columns <- sapply(data_Transformed, is.factor)

# Exclude the target variable from the numeric and factor columns
numeric_columns <- numeric_columns & names(data_Transformed) != "loan_status"

# Order the columns: first the numeric ones, then the factor ones, and finally the target variable
ordered_columns <- c(names(data_Transformed)[numeric_columns], 
                     names(data_Transformed)[factor_columns], 
                     "loan_status")

# Reorder DataFrame
data_Transformed <- data_Transformed[, ordered_columns]
```

### Data Normalization

We ensure that all the numerical variables are normalized to the same scale, so that thre isn't one with a
greater influence than the others.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Identificar las columnas numéricas
numerical_columns <- sapply(data_Transformed, is.numeric)

# Aplicar una transformación (por ejemplo, normalización) solo a las columnas numéricas
data_Normalized <- data_Transformed
## normalize the iris numerical data
data_Normalized[, numerical_columns] <- as.data.frame(lapply(data_Transformed[, numerical_columns], normalize))

# Confirm that normalization worked
summary(data_Normalized)
```

### Train-Test Split

```{r}
# Convertir las variables categóricas en variables dummy
data_Normalized <- model.matrix(~ . - 1, data = data_Normalized)

# Convertir el resultado a un data frame
data_Normalized <- as.data.frame(data_Normalized)
```

```{r}
## To reproduce the calculations
seeds <- c(1357, 2468, 3579, 4680, 5791)
set.seed(seeds[1])

## Create an index to partition the data set
ind <- sample(2, nrow(data_Normalized), replace=TRUE, prob=c(0.80, 0.20))

data_Normalized.train <- data_Normalized[ind==1,]
data_Normalized.test <- data_Normalized[ind==2,]
```

We have split the data into a training set and a test set, with 80% of the data in the training set and 20%
in the test set. Let's check that the partitions have been done correctly.

```{r}
nrow(data_Normalized)
nrow(data_Normalized.train)
nrow(data_Normalized.test)
```

## Classification using Nearest Neighbors

### Predictions

```{r}
## Build the classifier
data_Normalized.knn <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 3)
```

```{r}
CrossTable(x = data_Normalized.test$loan_status, y = data_Normalized.knn, prop.chisq=FALSE)
F1_Score(data_Normalized.knn, data_Normalized.test$loan_status, positive = "0")
```

Desde la tabla podemos ver que el modelo en lineas generales es preciso, con un margen de error del 0.108 (precisión del 89.2%). Por otro lado, si nos fijamos
en la columna de aceptados, la precisión es del 0.724, un valor bastante bajo. Esto quiere decir que este modelo puede ser mas propenso a rechazar préstamos a esas muestras donde debería aceptarlos.

Probaremos solo utilizando las variables numéricas a ver como se comporta el modelo.

```{r}
## Build the classifier
data_Normalized.knn2 <- knn(data_Normalized.train[, 1:5], data_Normalized.test[, 1:5], cl = data_Normalized.train[, 21], k = 3)
CrossTable(x = data_Normalized.test$loan_status, y = data_Normalized.knn2, prop.chisq=FALSE)
```

Como se puede ver, el modelo es menos preciso, lo que tiene sentido ya que las variables categóricas también influyen en la decisión de si se aprueba o no un préstamo.

Ahora probaremos esta vez con 2 ks diferentes, 5 y 2.

```{r}
## Build the classifier
data_Normalized.knn3 <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 2)
data_Normalized.knn4 <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 5)
CrossTable(x = data_Normalized.test$loan_status, y = data_Normalized.knn3, prop.chisq=FALSE)
CrossTable(x = data_Normalized.test$loan_status, y = data_Normalized.knn4, prop.chisq=FALSE)
```

En ambos nuevos modelos no vemos mejorías sustanciales como para cambiar el modelo original, además de que el modelo original es el que menos tiempo tarda en ejecutarse.

## Classification using Naive Bayes

```{r}
classifier.NB <- naiveBayes(x = data_Normalized.train[, 1:20], y = data_Normalized.train[, 21])
data_Normalized.NB <- predict(classifier.NB, data_Normalized.test[, 1:20])

CrossTable(x = data_Normalized.test$loan_status, y = data_Normalized.NB, prop.chisq=FALSE)
accuracyNB <- sum(data_Normalized.NB == data_Normalized.test$loan_status) / nrow(data_Normalized.test)
F1_Score(data_Normalized.NB, data_Normalized.test$loan_status, positive = "0")
```

In the case of the Naive Bayes model, we can see that the accuracy is around 10% lower than the KNN model, which is not a great result.
It should also be noted that, as we can see in the table, the model takes a more conservative approach when it comes to evaluating loan profiles, therefore it is less likely to approve the loan request.
Because of this, this model may be prefered to KNN in this case, as the bank .ay prefer taking a safer approach by rejecting more requests even though they should be valid rather than accepting more that shouldn't be valid

## Classification using Decision Trees

Para poder hacer uso de los árboles de decisión, necesitamos convertir las variables númericas en categóricas.

### Conversion of Numerical Variables 


```{r}
## Convertir las variables numéricas en categóricas, divididas en intervalos:
data_Mod$person_age <- cut(data$person_age, breaks = c(20, 25, 30, 40, 50), labels = c("20-25", "26-30", "31-40", "41-50"))
data_Mod$person_income <- cut(data$person_income, breaks = c(0, 50000, 100000, 150000, 200000, 250000), labels = c("0-50k", "50k-100k", "100k-150k", "150k-200k", "200k-250k"))
data_Mod$loan_amnt <- cut(data$loan_amnt, breaks = c(0, 5000, 10000, 20000, 35000), labels = c("0-5k", "5k-10k", "10k-20k", "20k-35k"))
data_Mod$loan_int_rate <- cut(data$loan_int_rate, breaks = c(5, 8.5, 11, 13, 20), labels = c("5-8.5", "8.5-11", "11-13", "13-20"))
data_Mod$loan_percent_income <- cut(data$loan_percent_income, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), labels = c("0-0.1", "0.1-0.2", "0.2-0.3", "0.3-0.4", "0.4,0.5"))

#Hacemos lo mismo con la variable objetivo:
data_Mod$loan_status <- as.factor(data_Mod$loan_status)
```

### ID3 Algorithm

```{r}
## Create an index to partition the data set
ind1 <- sample(2, nrow(data_Mod), replace = TRUE, prob = c(0.80, 0.20))
data_Mod.train1 <- data_Mod[ind1 == 1, ]
data_Mod.test1 <- data_Mod[ind1 == 2, ]

# Crear el modelo
ID3_1 <- J48(loan_status ~ person_age + person_income + loan_amnt + loan_int_rate + loan_percent_income, data = data_Mod.train1)
plot(ID3_1)
```

Como podemos observar nos hemos pasado creando categorías, provocando que se dividan los datos de forma excesiva y puede llevar a producir overfitting.
Por tanto, vamos a reducir el número de intervalos con tal de mejorarlo.

```{r}
data_Mod$person_age <- cut(data$person_age, 
                           breaks = c(20, 25, 30, 40, 50), 
                           labels = c("20-25", "26-30", "31-40", "41-50"))

data_Mod$person_income <- cut(data$person_income, 
                              breaks = c(0, 50000, 100000, 250000), 
                              labels = c("0-50k", "50k-100k", "+100k"))

data_Mod$loan_amnt <- cut(data$loan_amnt, 
                          breaks = c(0, 10000, 20000, 35000), 
                          labels = c("0-10k", "10k-20k", "+20k"))

data_Mod$loan_int_rate <- cut(data$loan_int_rate, breaks = c(5, 8.5, 13, 20), labels = c("5-8.5", "8.5-13", "13-20"))

data_Mod$loan_percent_income <- cut(data$loan_percent_income, 
                                    breaks = c(0, 0.1, 0.2, 0.5), 
                                    labels = c("0-0.1", "0.1-0.2", "+0.2"))
```

Para definir estos nuevos intervalos nos hemos centrado en los rangos donde había una mayor concentración de los datos.

```{r}
## Create an index to partition the data set
ind2 <- sample(2, nrow(data_Mod), replace = TRUE, prob = c(0.80, 0.20))
data_Mod.train2 <- data_Mod[ind2 == 1, ]
data_Mod.test2 <- data_Mod[ind2 == 2, ]

# Crear el modelo
ID3_2 <- J48(loan_status ~ person_age + person_income + loan_amnt + loan_int_rate + loan_percent_income, data = data_Mod.train2)
plot(ID3_2)
```

```{r}
summary(ID3_1)
```

Veamos como se comporta con el conjunto test

```{r}
data_Mod.ID31 <- predict(ID3_1, data_Mod.test1)
confusionMatrix(data_Mod.ID31, data_Mod.test1$loan_status)
```

Como vemos tenemos un accuracy del 80.6%, lo cual no está mal, pero podemos mejorar.

```{r}
summary(ID3_2)
data_Mod.ID32 <- predict(ID3_2, data_Mod.test2)
confusionMatrix(data_Mod.ID32, data_Mod.test2$loan_status)
```

Ahora añadiremos el resto de variables categóricas, basándonos en el segundo modelo, con intervalos más óptimos:


```{r}
# Crear el modelo
ID3_3 <- J48(loan_status ~ ., data = data_Mod.train2)

data_Mod.ID33 <- predict(ID3_3, data_Mod.test2)
confusionMatrix(data_Mod.ID33, data_Mod.test2$loan_status)
```

Como hemos podido ver, el modelo ha mejorado, pero creemos que se puede optimizar. Por tanto, vamos a ver que variables son las más importantes poder optimizar el modelo sin perjudicar la precisión.

```{r}
info_gain <- information.gain(loan_status ~ ., data = data_Mod)
print(info_gain)
```

Generamos un nuevo árbol solo con las variables que realmente tienen importancia

```{r}
# Crear modelo con las variables seleccionadas
ID3_Sel <- J48(loan_status ~ previous_loan_defaults_on_file + loan_percent_income + person_home_ownership + person_income + loan_amnt + loan_intent, 
               data = data_Mod.train2)

data_Mod.ID3Sel <- predict(ID3_Sel, data_Mod.test2)
confusionMatrix(data_Mod.ID3Sel, data_Mod.test2$loan_status)
F1_Score(data_Mod.ID3Sel, data_Mod.test2$loan_status, positive = "0")
```

Como podemos observar, el modelo es más óptimo y la precisión se ha mantenido en un 87%, lo cual es un buen resultado.


### C5.0 Algorithm

```{r}
C50 <- C5.0(loan_status ~ ., data = data_Mod.train2)
data_Mod.C50 <- predict(C50, data_Mod.test2)
confusionMatrix(data_Mod.C50, data_Mod.test2$loan_status)
F1_Score(data_Mod.C50, data_Mod.test2$loan_status, positive = "0")
```

No observamos ninguna diferencia con los resultados obtenidos con el ID3, por tanto, no hay ninguna mejora.


### CART Algorithm

```{r}
classifier.CART <- ctree(loan_status ~ ., data = data_Mod.train1)
data_Mod.CART <- predict(classifier.CART, data_Mod.test1)
confusionMatrix(data_Mod.CART, data_Mod.test1$loan_status)
F1_Score(data_Mod.CART, data_Mod.test1$loan_status, positive = "0")
```

The results are similar to the ID3 model, they are a bit better but with a small margin. These are good results, but we should keep checking every tree model to see which one is the best.

### Decision Tree using rpart

```{r}
classifier.rpart <- rpart(loan_status ~ ., data = data_Mod.train1, method = "class")
data_Mod.rpart <- predict(classifier.rpart, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.rpart, data_Mod.test1$loan_status)
F1_Score(data_Mod.rpart, data_Mod.test1$loan_status, positive = "0")
```

Yet again, the results given by the decision tree are similar to the ones given by previous tree models.
In this case, it can be seen that this model is very reluctant to approving loans, so it may be a good model to use if the bank wants to be more conservative, as it offers a good enough accuracy while outputing safer results. 

### Decision Tree using Tree

```{r}
classifier.tree <- tree(loan_status ~ ., data = data_Mod.train1)
data_Mod.tree <- predict(classifier.tree, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.tree, data_Mod.test1$loan_status)
F1_Score(data_Mod.tree, data_Mod.test1$loan_status, positive = "0")
```

## Classification without omitting variables

### Ordenar las variables

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)

# Identificar las columnas numéricas y de tipo factor
numeric_columns <- sapply(data, is.numeric)
factor_columns <- sapply(data, is.factor)

# Excluir la variable objetivo de las columnas numéricas y de tipo factor
numeric_columns <- numeric_columns & names(data) != "loan_status"

# Ordenar las columnas: primero las numéricas, luego las de tipo factor, y finalmente la variable objetivo
ordered_columns <- c(names(data)[numeric_columns], 
                     names(data)[factor_columns], 
                     "loan_status")

# Reordenar el data frame
data <- data[, ordered_columns]
```

### Data Normalization

```{r}
## create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Identificar las columnas numéricas
numerical_columns <- sapply(data, is.numeric)

# Aplicar una transformación (por ejemplo, normalización) solo a las columnas numéricas
data_Normalized <- data
## normalize the iris numerical data
data_Normalized[, numerical_columns] <- as.data.frame(lapply(data[, numerical_columns], normalize))
## confirm that normalization worked
summary(data_Normalized)
```

### Train-Test Split

```{r}
# Convertir las variables categóricas en variables dummy
data_Normalized <- model.matrix(~ . - 1, data = data_Normalized)

# Convertir el resultado a un data frame
data_Normalized <- as.data.frame(data_Normalized)
```

```{r}
## To reproduce the calculations
seeds <- c(1357, 2468, 3579, 4680, 5791)
set.seed(seeds[1])

## Create an index to partition the data set
ind <- sample(2, nrow(data_Normalized), replace=TRUE, prob=c(0.80, 0.20))

data_Normalized.train <- data_Normalized[ind==1,]
data_Normalized.test <- data_Normalized[ind==2,]
```

Veamos que se hayan hecho bien las particiones

```{r}
nrow(data_Normalized)
nrow(data_Normalized.train)
nrow(data_Normalized.test)
```

```{r}
data$person_age <- cut(data$person_age, 
                           breaks = c(20, 25, 30, 40, 50), 
                           labels = c("20-25", "26-30", "31-40", "41-50"))

data$person_income <- cut(data$person_income, 
                              breaks = c(0, 50000, 100000, 250000), 
                              labels = c("0-50k", "50k-100k", "+100k"))

data$loan_amnt <- cut(data$loan_amnt, 
                          breaks = c(0, 10000, 20000, 35000), 
                          labels = c("0-10k", "10k-20k", "+20k"))

data$loan_percent_income <- cut(data$loan_percent_income, 
                                    breaks = c(0, 0.1, 0.2, 0.5), 
                                    labels = c("0-0.1", "0.1-0.2", "+0.2"))

                                    
```

Para definir estos nuevos intervalos nos hemos centrado en los rangos donde había una mayor concentración de los datos.

```{r}
## Create an index to partition the data set
ind2 <- sample(2, nrow(data), replace = TRUE, prob = c(0.80, 0.20))
data.train2 <- data[ind2 == 1, ]
data.test2 <- data[ind2 == 2, ]

# Crear el modelo
ID3_2 <- J48(loan_status ~ person_age + person_income + loan_amnt + loan_percent_income, data = data.train2)
```

```{r}
summary(ID3_2)
```

Veamos como se comporta con el conjunto test

```{r}
data.ID32 <- predict(ID3_2, data.test2)
confusionMatrix(data.ID32, data.test2$loan_status)
```

## Conclusion