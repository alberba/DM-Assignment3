---
title: "Assignment 3"
author: "Santiago Rattenbach, Angel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(e1071)
    library(class)
    library(gmodels)
    library(tree)
    library(FSelector)
    library(partykit)
    library(party)
    library(RWeka)
    library(caret)
    library(C50)
    library(rpart.plot)
    library(MLmetrics)
  })
})
```

## The Data

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

### Independent Variables

As we can see, the dataset contains 45,000 observations and 14 variables. The variables are as follows:

1. **person age:** Age of the person (numeric)
2. **person gender:** Gender of the person (categorical: female, male)
3. **person education:** Highest education level (categorical: Associate, Bachelor, Doctorate, High School, 
Master)
4. **person income:** Annual income (numeric)
5. **person emp exp:** Years of employment experience (integer)
6. **person home ownership:** Home ownership status (categorical: MORTGAGE, OTHER, OWN, RENT)
7. **loan amnt:** Loan amount requested (numeric)
8. **loan intent:** Purpose of the loan (categorical: DEBTCONSOLIDATION, EDUCATION, HOME-IMPROVEMENT, 
MEDICAL, PERSONAL, VENTURE)
9. **loan int rate:** Loan interest rate (numeric)
10. **loan percent income:** Loan amount as a percentage of annual income (numeric)
11. **cb person cred hist length:** Length of credit history in years (numeric)
12. **credit score:** Credit score of the person (integer)
13. **previous loan defaults on file:** Indicator of previous loan defaults (categorical: No, Yes)

### Target Variable

- **loan_status**: The status of the loan (integer 1 = approved; 0 = rejected)

```{r}
# Crear el gráfico de barras
ggplot(data, aes(x = factor(loan_status, labels = c("Rechazado", "Aprobado")))) +
  geom_bar(fill = "skyblue") +
  labs(title = "Distribution of loan approval status",
       x = "Loan status",
       y = "Quantity")
```

Most loans end up getting denied, we aim to understand why that is the case and build a model that predicts 
the outcome as much as possible.

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```

#### Numerical Variables

- **person_age:** This dataset includes adults, so the minimum age is 20 years. The mean is 27.76 years, 
meaning most of the samples in the dataset are from young people. On the other hand, we can see that 
there are very few samples for people over 50 years old. Therefore, we will remove observations with 
ages above 50 as we consider them outliers.

```{r}
data <- subset(data, person_age <= 50)
```

- **person_income:** This dataset ranges from annual values of $8,000 to $7,200,766. We will assume 
that samples with annual incomes above $250,000 would not face any difficulty obtaining a loan within 
the scope of this dataset.

```{r}
data <- subset(data, person_income < 250000)
```

- **person_emp_exp:** This variable ranges from 0 to 123 years. The latter is quite an outlier, especially 
considering that the maximum accepted age is 50 years. Similarly, by removing observations with ages above 
50 years, we also eliminate observations with more than 50 years of work experience.

- **loan_amnt**: The loan amounts range from $500 to $35,000, with no apparent outliers at first glance.

- **loan_int_rate**: The loan interest rate ranges from 5.42% to 20%. It is likely that as the interest rate
 increases, the probability of the loan being approved also increases, since the bank would be taking on
 greater risk but could achieve higher profit.
 
- **loan_percent_income**: As we can see, there are samples where the loan percentage over annual income is 
0%, which is impossible. So, we will check how many samples meet this condition.


```{r}
aux <- subset(data, loan_percent_income == 0)
head(aux)
```

We see that there are 3 samples with a loan percentage over annual income of 0%. This is impossible because 
if the percentage is 0%, the loan amount would be $0. Therefore, we will proceed to remove them.

```{r}
data <- subset(data, loan_percent_income != 0)
```

- **cb_person_cred_hist_length**: The length of credit history ranges from 2 to 30 years. There don't seem 
to be any outliers.

- **credit_score**: The credit score ranges from 390 to 850. There don't seem to be any outliers.

Let's see how the dataset looks after removing the outliers:

```{r}
summary(data)
```
```{r}
# List of numerical variables:
numeric <- c('person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 
                    'loan_int_rate', 'loan_percent_income', 
                    'cb_person_cred_hist_length', 'credit_score')


# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

As we can see, except for the credit_score, most of the numerical variables do not follow a normal 
distribution and are right-skewed. Later, we'll modify the values to make them follow a normal distribution.

#### Categorical

```{r}
# List of categorical variables:
categories <- c('person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **gender:** As we can see, the majority of the applicants are male, however, the difference is not very
significant so there isn't much information to be gained from these values.

- **education:** The number of applicants with a master's degree is approximately half the number of people 
in any category, except for those with a doctorate, who are significantly fewer than that. This variable may 
be useful in predicting the loan status.
It can be observed that people who have a higher level of education are more.prone to apply for loans than
those with a lower level of education..

- **home ownership:** The majority of applicants are renters, followed by mortgage holders. There are few 
applicants who own their homes and even fewer who own other types of homes. This variable may also be useful 
in predicting the loan status.

- **loan intent:** The most common loan intents are both education and medical bills, followed by debt
consolidation, venture and personal loans. Fewer people take out loans for home improvement.
This variable may also be useful in predicting the loan status.

### Data Correlations

#### Numerical Variables

```{r}
numeric_Corr <- data[, c('person_age', 'person_income', 'person_emp_exp',
                        'loan_amnt', 'loan_int_rate', 'loan_percent_income',
                        'cb_person_cred_hist_length', 'credit_score', 'loan_status')]

ggcorr(numeric_Corr, label = TRUE)
```
As we can see, 'credit_score' has little influence, as its correlations with the other variables are 
virtually null, and it has no correlation with the target variable. Therefore, we will proceed to remove it. 
On the other hand, 'person_age', 'person_emp_exp', and 'cb_person_cred_hist_length' have a very high 
correlation with each other, so we will remove 'person_emp_exp' and 'cb_person_cred_hist_length' to avoid 
multicollinearity.

```{r}
# Supression of the variables
data_Mod <- data[, -c(5, 11, 12)]

# Verify the changes
names(data_Mod)
```

```{r}
# Compare the correlation matrix of the new dataset:
ggcorr(data_Mod, label = TRUE, label_round = 2)
```

Now we can see that all the correlations are appropriate, so we can proceed to the next stage.
 
### Bivariate Analysis

#### Numeric Variables

Comparing the loan status with the all numerical variables to see if there is any relationship between them.

```{r} 
p1 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = person_age), fill = "violet", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Person Age", x = "Loan Status", y = "Person Age") +
  theme_minimal()

print(p1)
```

As we can saw earlier and we can confirm with this graph, the age of the person does not seem to have a 
significant impact on the loan status, as the median age is very similar for both approved and rejected loans.

```{r}
p2 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = person_income), fill = "#FFB6C1", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Person Income", x = "Loan Status", y = "Person Income") +
  theme_minimal()

print(p2)
```

Loan approval is inversly proportional to the applicant's income, higher incomes tend to have their loans 
rejected. This information doesn't seem to make much sense to us, as you would think that loans taken by
people with higher income should be both safer and more profitable, however that is not the case here.


```{r}
p3 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_amnt), fill = "violet", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Amount", x = "Loan Status", y = "Loan Amount") +
  theme_minimal()

print(p3)
```

As we can see, those who have their loans approved tend to request higher loan amounts, that is, the higher 
the loan amount, the more likely it is to be approved. This makes sense as the bank would make more profit.

```{r}
p4 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_percent_income), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Percent Income", x = "Loan Status", y = "Loan Percent Income") +
  theme_minimal()

print(p4)
```

In this case, those who have a higher loan percentage over annual income are more likely to have their
loans approved. This is probably because the bank makes more profit from these loans with more interest rate, 
even though they are riskier than those with a lower percentage.

```{r}
p5 <- ggplot(data_Mod) + geom_boxplot(aes(x = factor(loan_status), y = loan_int_rate), fill = "#FFDAB9", alpha = 0.6) +
  theme(axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status vs Loan Int Rate", x = "Loan Status", y = "Loan Int Rate") +
  theme_minimal()

print(p5)
```


#### Categorical Variables

Comparing the loan status with the all categorical variables to see if there is any relationship between them.

```{r}
p1 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_gender, y = loan_status), fill = "lightblue") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Gender", x = "Person Gender", y = "Loan Status")

print(p1)
```

```{r}
p2 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_education, y = loan_status), fill = "lightpink") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Education", x = "Person Education", y = "Loan Status")

print(p2)
```

```{r}
p3 <- ggplot(data_Mod) +
  geom_violin(aes(x = person_home_ownership, y = loan_status), fill = "lightgreen") +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Person Home Ownership", x = "Person Home Ownership", y = "Loan Status")

print(p3)
```

```{r}
p4 <- ggplot(data_Mod) +
  geom_violin(aes(x = loan_intent, y = loan_status), fill = "lavender") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Loan Intent", x = "Loan Intent", y = "Loan Status")

print(p4)
```

```{r}
p5 <- ggplot(data_Mod) +
  geom_violin(aes(x = previous_loan_defaults_on_file, y = loan_status), fill = "peachpuff") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 8.5)) +
  labs(title = "Loan Status by Previous Loan Defaults", x = "Previous Loan Defaults", y = "Loan Status")

print(p5)
```

It can be seen that the variable 'previous_loan_defaults_on_file' is key to predicting the loan status, as
applicants who have had previous loan defaults are always rejected. On the other hand, the rest of the
attributes do not seem to have a significant impact on the loan status (at least not easily seen via this
plots) regardless of their value, as all of their profiles are accepted around 25% of the times.

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data_Mod))
```

As we can see, there are no missing values in the dataset, so we can proceed to the next stage.

### Key Features

As we have already seen with the vioin plots, the 'previous_loan_defaults_on_file' variable is key to
predicting the loan status, as applicants who have had previous loan defaults are always rejected.

Analyzing the correlation between the variables, it is observed that both person_income and loan_amnt,
loan_int_rate, and loan_percent_income have a high correlation, therefore they are also important
variables to consider.

### PreProcessing Data

#### Checking Normality (Skewness)

Now we will check the normality of the numerical variables to see if they follow a normal distribution.
In case they do not, we will apply transformations to make them follow a normal distribution.

- **person_age**:
  
```{r}
skewness(data_Mod$person_age)
```

We get a skewness of 1.37, so we need to apply a transformation to improve the distribution.

```{r}
skewness(log(data_Mod$person_age - 19))
```

We see that by applying this logarithmic transformation, we improve the distribution of the 'person_age'
variable, since the minimum age of the people is 20 years, so we subtract 19 to avoid the logarithm of 0.

- **person_income**:
  
```{r}
skewness(data_Mod$person_income)
```

We get a skewness of 1.31, so we need to apply another transformation, to improve the distribution.

```{r}
skewness(log(data_Mod$person_income))
```

We see that by applying this logarithmic transformation, we improve the distribution of the 'person_income'
variable, going from 1.31 to -0.18, which is a significant improvement.

- **loan_amnt**:

```{r}
skewness(data_Mod$loan_amnt)
```

We get a skewness of 1.17, so we need to apply a transformation to improve it.

```{r}
skewness(sqrt(data_Mod$loan_amnt))
skewness(log(data_Mod$loan_amnt))
```

As we can see, both solve the asymmetry problem, but the square root transformation is more effective.

- **loan_int_rate**:

```{r}
skewness(data_Mod$loan_int_rate)
```

We get a skewness of 0.21, so we do not need to apply any transformation.

- **loan_percent_income**:

```{r}
skewness(data_Mod$loan_percent_income)
```

We get a skewness of 1.03, so we need to apply another transformation.

```{r}
skewness(sqrt(data_Mod$loan_percent_income))
skewness(log(data_Mod$loan_percent_income))
```

As we can see, both solve the skewness problem, but the square root transformation is more effective.

#### Creation of a better skewed dataset

With all the transformations made, we proceed to create a new dataset with the transformed variables,
as they better follow the normal distribution:

```{r}
data_Transformed <- data_Mod

data_Transformed$person_age <- log(data_Mod$person_age - 19)
data_Transformed$person_income <- log(data_Mod$person_income)
data_Transformed$loan_amnt <- sqrt(data_Mod$loan_amnt)
data_Transformed$loan_percent_income <- sqrt(data_Mod$loan_percent_income)
```

## Model Building

### Sorting Variables

To better handle the data, we find it convenient to reorder the columns, placing the numerical columns 
first, followed by the categorical ones, and finally the target variable.

```{r}
# Identify numeric and factor columns
numeric_columns <- sapply(data_Transformed, is.numeric)
factor_columns <- sapply(data_Transformed, is.factor)

# Exclude the target variable from the numeric and factor columns
numeric_columns <- numeric_columns & names(data_Transformed) != "loan_status"

# Order the columns: first the numeric ones, then the factor ones, and finally the target variable
ordered_columns <- c(names(data_Transformed)[numeric_columns], 
                     names(data_Transformed)[factor_columns], 
                     "loan_status")

# Reorder DataFrame
data_Transformed <- data_Transformed[, ordered_columns]
```

### Data Normalization

We ensure that all the numerical variables are normalized to the same scale, so that there isn't one with a
greater influence than the others.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Identificar las columnas numéricas
numerical_columns <- sapply(data_Transformed, is.numeric)

# Aplicar una transformación (por ejemplo, normalización) solo a las columnas numéricas
data_Normalized <- data_Transformed
## normalize the iris numerical data
data_Normalized[, numerical_columns] <- as.data.frame(lapply(data_Transformed[, numerical_columns], normalize))

# Confirm that normalization worked
summary(data_Normalized)
```

### Train-Test Split

Before splitting into train and test sets, it is important to know that for k-Nearest Neighbors and Naive 
Bayes, all variables need to be numeric. For categorical variables, we use one-hot encoding to convert them 
into numeric variables, where each category becomes a new binary column.

```{r}
# Convertir las variables categóricas en variables one hot encoding
data_Normalized <- model.matrix(~ . - 1, data = data_Normalized)

# Convertir el resultado a un data frame
data_Normalized <- as.data.frame(data_Normalized)
```

```{r}
## To reproduce the calculations
seeds <- c(1357, 2468, 3579, 4680, 5791)
set.seed(seeds[1])

## Create an index to partition the data set
ind <- sample(2, nrow(data_Normalized), replace=TRUE, prob=c(0.80, 0.20))

data_Normalized.train <- data_Normalized[ind==1,]
data_Normalized.test <- data_Normalized[ind==2,]
```

We have split the data into a training set and a test set, with 80% of the data in the training set and 20%
in the test set. Let's check that the partitions have been done correctly.

```{r}
nrow(data_Normalized)
nrow(data_Normalized.train)
nrow(data_Normalized.test)
```

## Classification using Nearest Neighbors

### Predictions

We will start building the KNN model, where the independent variables range from position 1 to 20, and 
loan_status is in position 21. We have chosen k=3 as the default value.

```{r}
## Build the classifier
data_Normalized.knn <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 3)

confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn)
F1_Score(data_Normalized.knn, data_Normalized.test$loan_status, positive = "0")
```

From the table, we can see that the model is generally accurate, with an error margin of 0.108 (89.2% 
accuracy). On the other hand, if we focus on the 'accepted' column, the accuracy is 0.724, which is quite 
low. This means that the model may be more prone to approving loans for cases where it should reject them. 
This could have a **negative impact on the bank**, as it would have to take on more risks.

To understand why it is important to convert categorical variables into numerical ones instead of ignoring 
them, we will create a model using only the numerical variables. These range from the first to the fifth 
column.

```{r}
## Build the classifier
data_Normalized.knn2 <- knn(data_Normalized.train[, 1:5], data_Normalized.test[, 1:5], cl = data_Normalized.train[, 21], k = 3)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn2)
```

As we can see, the model is less accurate, which makes sense since the categorical variables also influence 
the decision of whether a loan is approved or not.

Now, we will try with two different k values: 5 and 2.

```{r}
## Build the classifier
data_Normalized.knn3 <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 2)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn3)
```

```{r}
data_Normalized.knn4 <- knn(data_Normalized.train[, 1:20], data_Normalized.test[, 1:20], cl = data_Normalized.train[, 21], k = 5)
confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.knn4)
```

In both new models, we don't see any substantial improvements that would justify changing the original model. 
Additionally, the original model is the one that takes the least time to execute.

## Classification using Naive Bayes

```{r}
classifier.NB <- naiveBayes(x = data_Normalized.train[, 1:20], y = data_Normalized.train[, 21])
data_Normalized.NB <- predict(classifier.NB, data_Normalized.test[, 1:20])

confusionMatrix(as.factor(data_Normalized.test$loan_status), data_Normalized.NB)
F1_Score(data_Normalized.NB, data_Normalized.test$loan_status, positive = "0")
```

In the case of the Naive Bayes model, we can see that the accuracy is around 10% lower than the KNN model, 
which is not a great result. This may be due to the fact that in the Naive Bayes model, it is assumed that
the variables are conditionally independent, when in reality they are not.

It should also be noted that, as we can see in the table, the model takes a more conservative approach when 
it comes to evaluating loan profiles, therefore it is less likely to approve the loan request. Because of 
this, this model may be prefered to KNN in this case, as the bank may prefer taking a safer approach by 
rejecting more requests even though they should be valid rather than accepting more that shouldn't be valid

## Classification using Decision Trees

In order to use decision trees, we need to convert the numerical variables into categorical ones.

### Conversion of Numerical Variables 

```{r}
## Convertir las variables numéricas en categóricas, divididas en intervalos:
data_Mod$person_age <- cut(data$person_age, breaks = c(19, 25, 30, 40, 51), labels = c("20-25", "26-30", "31-40", "41-50"))
data_Mod$person_income <- cut(data$person_income, breaks = c(0, 50000, 100000, 150000, 200000, 250000), labels = c("0-50k", "50k-100k", "100k-150k", "150k-200k", "200k-250k"))
data_Mod$loan_amnt <- cut(data$loan_amnt, breaks = c(0, 5000, 10000, 20000, 35000), labels = c("0-5k", "5k-10k", "10k-20k", "20k-35k"))
data_Mod$loan_int_rate <- cut(data$loan_int_rate, breaks = c(5, 8.5, 11, 13, 20), labels = c("5-8.5", "8.5-11", "11-13", "13-20"))
data_Mod$loan_percent_income <- cut(data$loan_percent_income, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.67), labels = c("0-0.1", "0.1-0.2", "0.2-0.3", "0.3-0.4", "0.4-0.67"))

#Hacemos lo mismo con la variable objetivo:
data_Mod$loan_status <- as.factor(data_Mod$loan_status)
```

### ID3 Algorithm

```{r}
## Create an index to partition the data set
ind1 <- sample(2, nrow(data_Mod), replace = TRUE, prob = c(0.80, 0.20))
data_Mod.train1 <- data_Mod[ind1 == 1, ]
data_Mod.test1 <- data_Mod[ind1 == 2, ]

# Crear el modelo
ID3_1 <- J48(loan_status ~ person_age + person_income + loan_amnt + loan_int_rate + loan_percent_income, data = data_Mod.train1)
```

Once we have built the tree, let's see how it performs with the test set.

```{r}
data_Mod.ID31 <- predict(ID3_1, data_Mod.test1)
confusionMatrix(data_Mod.ID31, data_Mod.test1$loan_status)
```

As we can see, we have an accuracy of 81.5%, which is not bad, but there is room for improvement.

Now, we will add the rest of the categorical variables, based on the second model, with more optimal 
intervals:


```{r}
# Crear el modelo
ID3_2 <- J48(loan_status ~ ., data = data_Mod.train1)

data_Mod.ID32 <- predict(ID3_2, data_Mod.test1)
confusionMatrix(data_Mod.ID32, data_Mod.test1$loan_status)
F1_Score(data_Mod.ID32, data_Mod.test1$loan_status, positive = "0")
```

As we have seen, the model has improved significantly. On the other hand, we believe the tree may be 
overloaded with nodes, so let's see which variables are the most important in order to optimize the model 
without compromising accuracy.

```{r}
information.gain(loan_status ~ ., data = data_Mod)
```

We generated a new tree using only the variables that are truly important. In this case, we have removed the 
4 variables with the least importance (person_age, person_gender, person_education, and loan_amnt):

```{r}
# Crear modelo con las variables seleccionadas
ID2_Sel <- J48(loan_status ~ previous_loan_defaults_on_file + loan_percent_income + person_home_ownership + person_income + loan_intent, 
               data = data_Mod.train1)

data_Mod.ID2Sel <- predict(ID2_Sel, data_Mod.test1)
confusionMatrix(data_Mod.ID2Sel, data_Mod.test1$loan_status)
F1_Score(data_Mod.ID2Sel, data_Mod.test1$loan_status, positive = "0")
```

The accuracy has dropped to 87.3%, a decrease of 3%. Nevertheless, we believe the model has been 
significantly optimized, as by removing 4 variables, it produces far fewer nodes. Depending on the purpose,
a 3% drop may not be worth it.


### C5.0 Algorithm

```{r}
C50 <- C5.0(loan_status ~ ., data = data_Mod.train1)
data_Mod.C50 <- predict(C50, data_Mod.test1)
confusionMatrix(data_Mod.C50, data_Mod.test1$loan_status)
F1_Score(data_Mod.C50, data_Mod.test1$loan_status, positive = "0")
```

We observe a slight improvement compared to the ID3 model, but it is not significant. This may be due to the 
fact that not all partitions are distributed in the same way, so the C5.0 model may benefit in this case. 
However, the improvement is almost negligible.


### CART Algorithm

```{r}
classifier.CART <- ctree(loan_status ~ ., data = data_Mod.train1)
data_Mod.CART <- predict(classifier.CART, data_Mod.test1)
confusionMatrix(data_Mod.CART, data_Mod.test1$loan_status)
F1_Score(data_Mod.CART, data_Mod.test1$loan_status, positive = "0")
```

The results are similar to those obtained with C5.0; they are slightly worse, but with a very small margin.

### Decision Tree using rpart

```{r}
classifier.rpart <- rpart(loan_status ~ ., data = data_Mod.train1, method = "class")
data_Mod.rpart <- predict(classifier.rpart, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.rpart, data_Mod.test1$loan_status)
F1_Score(data_Mod.rpart, data_Mod.test1$loan_status, positive = "0")
```

Yet again, the results given by the decision tree are similar to the ones given by previous tree models.
In this case, it can be seen that this model is very reluctant to approving loans, so it may be a good model to use if the bank wants to be more conservative, as it offers a good enough accuracy while outputing safer results. 

### Decision Tree using Tree

```{r}
classifier.tree <- tree(loan_status ~ ., data = data_Mod.train1)
data_Mod.tree <- predict(classifier.tree, data_Mod.test1, type = "class")
confusionMatrix(data_Mod.tree, data_Mod.test1$loan_status)
F1_Score(data_Mod.tree, data_Mod.test1$loan_status, positive = "0")
```

The results are still quite similar, although this model performs slightly worse than the previous ones.

## Classification without omitting variables

To conduct more tests, let's check if what we did earlier by removing variables that had a high correlation
with other independent variables was correct. For that, we will compare one of the previous models, but 
without omitting any variables.

### Ordenar las variables

```{r}
data <- read.csv("./loan_data.csv", header=TRUE, stringsAsFactors=TRUE)

# Identificar las columnas numéricas y de tipo factor
numerical_columns <- sapply(data, is.numeric)
factor_columns <- sapply(data, is.factor)

# Excluir la variable objetivo de las columnas numéricas y de tipo factor
numerical_columns <- numerical_columns & names(data) != "loan_status"

# Ordenar las columnas: primero las numéricas, luego las de tipo factor, y finalmente la variable objetivo
ordered_columns <- c(names(data)[numerical_columns], 
                     names(data)[factor_columns], 
                     "loan_status")

# Reordenar el data frame
data <- data[, ordered_columns]
```

### Data Normalization

```{r}
## create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

numerical_columns <- sapply(data, is.numeric)
# Aplicar una transformación (por ejemplo, normalización) solo a las columnas numéricas
data_NoModified <- data
## normalize the iris numerical data
data_NoModified[, numerical_columns] <- as.data.frame(lapply(data[, numerical_columns], normalize))
## confirm that normalization worked
summary(data_NoModified)
```

### Train-Test Split

```{r}
# Convertir las variables categóricas en variables dummy
data_NoModified <- model.matrix(~ . - 1, data = data_NoModified)

# Convertir el resultado a un data frame
data_NoModified <- as.data.frame(data_NoModified)
```

```{r}
## To reproduce the calculations
seeds <- c(1357, 2468, 3579, 4680, 5791)
set.seed(seeds[1])

## Create an index to partition the data set
ind <- sample(2, nrow(data_NoModified), replace=TRUE, prob=c(0.80, 0.20))

data_NoModified.train <- data_NoModified[ind==1,]
data_NoModified.test <- data_NoModified[ind==2,]
```

```{r}
nrow(data_NoModified)
nrow(data_NoModified.train)
nrow(data_NoModified.test)
```

```{r}
## Build the classifier
data_NoModified.knn <- knn(data_NoModified.train[, 1:23], data_NoModified.test[, 1:23], cl = data_NoModified.train[, 24], k = 3)

confusionMatrix(as.factor(data_NoModified.test$loan_status), data_NoModified.knn)
F1_Score(data_NoModified.knn, data_NoModified.test$loan_status, positive = "0")
```

As we can see, both the F1 score and accuracy decrease, and the model itself is less efficient. By including 
variables with high correlation, unnecessary data is added, which slows down the model.

## Conclusion